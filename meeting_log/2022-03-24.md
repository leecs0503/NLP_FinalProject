<<<<<<< HEAD

2022-03-24 논의
===

[논문 2](https://arxiv.org/pdf/2202.03052v1.pdf)

pretraining dataset이 각 task로부터 데이터를 조금씩 가지고 온 상황

전체 데이터에서 적절히 일부를 뽑는 작업

->

~ 코드 살펴보는 중~

지금 NLP쪽 사전지식이 너무 전무한 상태

S2S
 - Encode
 - Decoder
 - Generator
 - Attention
 - Masking
 - Input Feeding
 - Teacher Forcing
 - (Encoder, Attention, Decoder, Generator, 구현하기)

Beam Search
 - 개념소개 및 구현

Transformer
 - Multi-head Attention
 - Encoder
 - Decoder with Masking
 - Positional Encoding
 - Learning rate warm =-up and linear decay
 - Appendix Beyond the paper

Bert

fairseq
=======

2022-03-24 논의
===

[논문 2](https://arxiv.org/pdf/2202.03052v1.pdf)

pretraining dataset이 각 task로부터 데이터를 조금씩 가지고 온 상황

전체 데이터에서 적절히 일부를 뽑는 작업

->

~ 코드 살펴보는 중~

지금 NLP쪽 사전지식이 너무 전무한 상태

S2S
 - Encode
 - Decoder
 - Generator
 - Attention
 - Masking
 - Input Feeding
 - Teacher Forcing
 - (Encoder, Attention, Decoder, Generator, 구현하기)

Beam Search
 - 개념소개 및 구현

Transformer
 - Multi-head Attention
 - Encoder
 - Decoder with Masking
 - Positional Encoding
 - Learning rate warm =-up and linear decay
 - Appendix Beyond the paper

Bert

fairseq
>>>>>>> initial commit
(https://fairseq.readthedocs.io/en/latest/)